llm:
  default_provider: "openai"  # Options: ollama, openai
  default_openai_model: "gpt-5-nano"  # Default OpenAI model for cost optimization
  model_selection_strategy: "cost-optimized"  # Options: cost-optimized, balanced, accuracy-first, manual
  
  ollama:
    base_url: "http://localhost:11434"
    model: "deepseek-r1:8b"  # Use smaller model for faster response
    timeout: 120
    temperature: 0.1
  
  openai:
    # Legacy single model config (maintained for backwards compatibility)
    model: "gpt-4o-mini"
    max_tokens: 2000
    temperature: 0.1
    timeout: 600  # 10 minute timeout for API calls (inference can be slow)
    
    # New multi-model configurations
    models:
      "gpt-5-nano":
        max_tokens: 2000
        temperature: 0.1
        pricing:
          input_per_1m: 0.05   # $0.05 per 1M input tokens (planned pricing)
          output_per_1m: 0.40  # $0.40 per 1M output tokens
        confidence_multiplier: 0.85  # Lower confidence for cheaper model
        complexity_threshold: 0.3    # Use for simple extractions (0.0-1.0)
        
      "gpt-5-mini":
        max_tokens: 2000
        temperature: 0.1
        pricing:
          input_per_1m: 0.25   # $0.25 per 1M input tokens (planned pricing)
          output_per_1m: 2.00  # $2.00 per 1M output tokens
        confidence_multiplier: 0.95  # Good balance of cost/accuracy
        complexity_threshold: 0.7    # Use for medium complexity extractions
        
      "gpt-5":
        max_tokens: 2000
        temperature: 0.1
        pricing:
          input_per_1m: 1.25   # $1.25 per 1M input tokens (planned pricing)
          output_per_1m: 10.00 # $10.00 per 1M output tokens
        confidence_multiplier: 1.0   # Highest accuracy baseline
        complexity_threshold: 1.0    # Use for complex extractions
    
    # Cost management
    cost_limits:
      max_cost_per_extraction: 0.10   # Maximum cost per single extraction
      max_daily_cost: 50.00           # Daily spending limit
      warn_threshold: 0.05            # Warn when extraction costs above this
    
    # API key will be read from OPENAI_API_KEY environment variable

processing:
  test_mode: false
  test_limit: 1
  batch_size: 1
  delay_between_requests: 1.0
  force_reprocess: false
  strict_validation: false
  
  # Model selection and benchmarking
  benchmark_sample_size: 50           # Number of records for benchmarking
  auto_fallback: true                 # Automatically use higher tier model if confidence too low
  fallback_confidence_threshold: 0.6  # Threshold to trigger model fallback

quality:
  min_confidence_threshold: 0.5
  review_threshold: 0.7

output:
  directory: "output/extracted"
  format: "yaml"

logging:
  level: "INFO"
  console: false
  file: true